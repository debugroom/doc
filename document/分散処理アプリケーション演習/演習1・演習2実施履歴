[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop
Usage: hadoop [--config confdir] COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  mradmin              run a Map-Reduce admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  fetchdt              fetch a delegation token from the NameNode
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce jobs
  queue                get information regarding JobQueues
  version              print the version
  jar <jar>            run a jar file
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  oiv                  apply the offline fsimage viewer to an fsimage
  classpath            prints the class path needed to get the
  dfsgroups            get the groups which users belong to on the Name Node
  mrgroups             get the groups which users belong to on the Job Tracker
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print help when invoked w/o parameters.
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs
Usage: java FsShell
           [-ls <path>]
           [-lsr <path>]
           [-df [<path>]]
           [-du <path>]
           [-dus <path>]
           [-count[-q] <path>]
           [-mv <src> <dst>]
           [-cp <src> <dst>]
           [-rm [-skipTrash] <path>]
           [-rmr [-skipTrash] <path>]
           [-expunge]
           [-put <localsrc> ... <dst>]
           [-copyFromLocal <localsrc> ... <dst>]
           [-moveFromLocal <localsrc> ... <dst>]
           [-get [-ignoreCrc] [-crc] <src> <localdst>]
           [-getmerge <src> <localdst> [addnl]]
           [-cat <src>]
           [-text <src>]
           [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>]
           [-moveToLocal [-crc] <src> <localdst>]
           [-mkdir <path>]
           [-setrep [-R] [-w] <rep> <path/file>]
           [-touchz <path>]
           [-test -[ezd] <path>]
           [-stat [format] <path>]
           [-tail [-f] <file>]
           [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
           [-chown [-R] [OWNER][:[GROUP]] PATH...]
           [-chgrp [-R] GROUP PATH...]
           [-help [cmd]]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|jobtracker:port>    specify a job tracker
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]

[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -lsr
lsr: Cannot access .: No such file or directory.
[root@hdclient01 work]# hadoop fs -lsr /
drwxr-xr-x   - hbase hadoop              0 2012-12-02 22:33 /hbase
drwxr-xr-x   - hdfs  supergroup          0 2012-12-02 22:32 /mapred
drwxrwxrwx   - mapred hadoop          0 2012-12-02 22:32 /mapred/staging
drwx------   - mapred hadoop          0 2012-12-02 22:33 /mapred/system
-rw-------   3 mapred hadoop          4 2012-12-02 22:33 /mapred/system/jobtracker.info
drwxrwxrwx   - hdfs  hadoop              0 2012-12-02 22:32 /tmp
drwxrwxrwx   - hdfs  hadoop              0 2012-12-02 22:32 /user
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# ls -l /root/hadoop_exercise/01/enwiki/
合計 152976
-rw-r--r-- 1 root root 155337545  3月  2  2012 enwiki-long1.xml
-rw-r--r-- 1 root root   1141099  3月  2  2012 enwiki-short1.xml
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -put /root/hadoop_exercise/01/enwiki /enwiki
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -lsr /enwiki
-rw-r--r--   3 root supergroup  155337545 2012-12-02 22:44 /enwiki/enwiki-long1.xml
-rw-r--r--   3 root supergroup    1141099 2012-12-02 22:44 /enwiki/enwiki-short1.xml
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -du /enwiki
Found 2 items
155337545   hdfs://hdmaster01:54312/enwiki/enwiki-long1.xml
1141099     hdfs://hdmaster01:54312/enwiki/enwiki-short1.xml
[root@hdclient01 work]# hadoop fs -dus /enwiki
hdfs://hdmaster01:54312/enwiki	156478644
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -cat /enwiki/enwiki-long1.xml | head -n 10
<feed>
<doc>
<title>Wikipedia: Cherry Grove Township, Minnesota</title>
<url>http://en.wikipedia.org/wiki/Cherry_Grove_Township,_Minnesota</url>
<abstract>Cherry Grove Township, Minnesota may refer to:</abstract>
<links>
<sublink linktype="nav"><anchor>All article disambiguation pages</anchor><link>http://en.wikipedia.org/wiki/Category:All_article_disambiguation_pages</link></sublink>
<sublink linktype="nav"><anchor>All disambiguation pages</anchor><link>http://en.wikipedia.org/wiki/Category:All_disambiguation_pages</link></sublink>
<sublink linktype="nav"><anchor>Article Feedback Blacklist</anchor><link>http://en.wikipedia.org/wiki/Category:Article_Feedback_Blacklist</link></sublink>
<sublink linktype="nav"><anchor>Disambiguation pages</anchor><link>http://en.wikipedia.org/wiki/Category:Disambiguation_pages</link></sublink>
cat: Unable to write to output stream.
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -cat /enwiki/enwiki-short1.xml | less
cat: Unable to write to output stream.
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -cp /enwiki /enwiki_backup
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -lsr /enwiki_backup
-rw-r--r--   3 root supergroup  155337545 2012-12-02 22:46 /enwiki_backup/enwiki-long1.xml
-rw-r--r--   3 root supergroup    1141099 2012-12-02 22:47 /enwiki_backup/enwiki-short1.xml
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -get /enwiki_backup /root/hadoop_exercise/01/enwiki_backup
[root@hdclient01 work]# ls
[root@hdclient01 work]# ls /root/hadoop_exercise/01/
enwiki	enwiki_backup
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -rmr /enwiki_backup
Moved to trash: hdfs://hdmaster01:54312/enwiki_backup
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# hadoop fs -ls /
Found 5 items
drwxr-xr-x   - root  supergroup          0 2012-12-02 22:44 /enwiki
drwxr-xr-x   - hbase hadoop              0 2012-12-02 22:33 /hbase
drwxr-xr-x   - hdfs  supergroup          0 2012-12-02 22:32 /mapred
drwxrwxrwx   - hdfs  hadoop              0 2012-12-02 22:32 /tmp
drwxrwxrwx   - hdfs  hadoop              0 2012-12-02 22:48 /user
[root@hdclient01 work]# rm -rf /root/hadoop_exercise/01/enwiki_backup/
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# ls /root/hadoop_exercise/01/
enwiki
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# ls
[root@hdclient01 work]# 
[root@hdclient01 work]# 
[root@hdclient01 work]# cd ..
[root@hdclient01 hadoop_exercise]# ls
01  02	04  06	07  11	12  14	work
[root@hdclient01 hadoop_exercise]# 
[root@hdclient01 hadoop_exercise]# 
[root@hdclient01 hadoop_exercise]# cd 02/
[root@hdclient01 02]# ls
wordcount
[root@hdclient01 02]# 
[root@hdclient01 02]# 
[root@hdclient01 02]# cd wordcount/
[root@hdclient01 wordcount]# ls -l
合計 4
-rw-r--r-- 1 root root 2151  3月  5  2012 WordCount.java
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# less WordCount.java 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# mkdir wordcount_classes
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# javac -classpath /usr/lib/hadoop-0.20/hadoop-core.jar -d wordcount_classes WordCount.java 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# ls -l wordcount_classes/
合計 12
-rw-r--r-- 1 root root 2208 12月  2 22:52 WordCount$Map.class
-rw-r--r-- 1 root root 1623 12月  2 22:52 WordCount$Reduce.class
-rw-r--r-- 1 root root 1490 12月  2 22:52 WordCount.class
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# jar cvf wordcount.jar -C wordcount_classes .
マニフェストが追加されました。
WordCount$Reduce.class を追加中です。(入 = 1623) (出 = 680)(58% 収縮されました)
WordCount$Map.class を追加中です。(入 = 2208) (出 = 962)(56% 収縮されました)
WordCount.class を追加中です。(入 = 1490) (出 = 747)(49% 収縮されました)
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop jar wordcount.jar WordCount /enwiki/enwiki-short1.xml /wordcount01
12/12/02 22:53:26 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
12/12/02 22:53:26 INFO input.FileInputFormat: Total input paths to process : 1
12/12/02 22:53:26 WARN snappy.LoadSnappy: Snappy native library is available
12/12/02 22:53:26 INFO util.NativeCodeLoader: Loaded the native-hadoop library
12/12/02 22:53:26 INFO snappy.LoadSnappy: Snappy native library loaded
12/12/02 22:53:27 INFO mapred.JobClient: Running job: job_201212022233_0001
12/12/02 22:53:28 INFO mapred.JobClient:  map 0% reduce 0%
12/12/02 22:53:39 INFO mapred.JobClient:  map 100% reduce 0%
12/12/02 22:53:51 INFO mapred.JobClient:  map 100% reduce 100%
12/12/02 22:53:53 INFO mapred.JobClient: Job complete: job_201212022233_0001
12/12/02 22:53:53 INFO mapred.JobClient: Counters: 26
12/12/02 22:53:53 INFO mapred.JobClient:   Job Counters 
12/12/02 22:53:53 INFO mapred.JobClient:     Launched reduce tasks=1
12/12/02 22:53:53 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=10275
12/12/02 22:53:53 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
12/12/02 22:53:53 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
12/12/02 22:53:53 INFO mapred.JobClient:     Launched map tasks=1
12/12/02 22:53:53 INFO mapred.JobClient:     Data-local map tasks=1
12/12/02 22:53:53 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=11790
12/12/02 22:53:53 INFO mapred.JobClient:   FileSystemCounters
12/12/02 22:53:53 INFO mapred.JobClient:     FILE_BYTES_READ=348427
12/12/02 22:53:53 INFO mapred.JobClient:     HDFS_BYTES_READ=1141212
12/12/02 22:53:53 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=795747
12/12/02 22:53:53 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=90933
12/12/02 22:53:53 INFO mapred.JobClient:   Map-Reduce Framework
12/12/02 22:53:53 INFO mapred.JobClient:     Map input records=15427
12/12/02 22:53:53 INFO mapred.JobClient:     Reduce shuffle bytes=348427
12/12/02 22:53:53 INFO mapred.JobClient:     Spilled Records=56836
12/12/02 22:53:53 INFO mapred.JobClient:     Map output bytes=291585
12/12/02 22:53:53 INFO mapred.JobClient:     CPU time spent (ms)=4350
12/12/02 22:53:53 INFO mapred.JobClient:     Total committed heap usage (bytes)=197070848
12/12/02 22:53:53 INFO mapred.JobClient:     Combine input records=0
12/12/02 22:53:53 INFO mapred.JobClient:     SPLIT_RAW_BYTES=113
12/12/02 22:53:53 INFO mapred.JobClient:     Reduce input records=28418
12/12/02 22:53:53 INFO mapred.JobClient:     Reduce input groups=8864
12/12/02 22:53:53 INFO mapred.JobClient:     Combine output records=0
12/12/02 22:53:53 INFO mapred.JobClient:     Physical memory (bytes) snapshot=377151488
12/12/02 22:53:53 INFO mapred.JobClient:     Reduce output records=8864
12/12/02 22:53:53 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=2467540992
12/12/02 22:53:53 INFO mapred.JobClient:     Map output records=28418
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -lsr /wordcount01
-rw-r--r--   3 root supergroup          0 2012-12-02 22:53 /wordcount01/_SUCCESS
drwxr-xr-x   - root supergroup          0 2012-12-02 22:53 /wordcount01/_logs
drwxr-xr-x   - root supergroup          0 2012-12-02 22:53 /wordcount01/_logs/history
-rw-r--r--   3 root supergroup      42984 2012-12-02 22:53 /wordcount01/_logs/history/hdmaster01_1354455187902_job_201212022233_0001_conf.xml
-rw-r--r--   3 root supergroup      12296 2012-12-02 22:53 /wordcount01/_logs/history/hdmaster01_1354455187902_job_201212022233_0001_root_wordcount
-rw-r--r--   3 root supergroup      90933 2012-12-02 22:53 /wordcount01/part-r-00000
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoo fs -cat /wordcount01/part-r-00000 | less
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -cat /wordcount01/part-r-00000 | less
cat: Unable to write to output stream.
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -cat /wordcount01/part-r-00000 | sort -k2 -n -r | less
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop jar wordcount.jar WordCount /enwiki/enwiki-long1.xml /wordcount02
12/12/02 22:57:34 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
12/12/02 22:57:34 INFO input.FileInputFormat: Total input paths to process : 1
12/12/02 22:57:34 WARN snappy.LoadSnappy: Snappy native library is available
12/12/02 22:57:34 INFO util.NativeCodeLoader: Loaded the native-hadoop library
12/12/02 22:57:34 INFO snappy.LoadSnappy: Snappy native library loaded
12/12/02 22:57:35 INFO mapred.JobClient: Running job: job_201212022233_0002
12/12/02 22:57:36 INFO mapred.JobClient:  map 0% reduce 0%
12/12/02 22:57:50 INFO mapred.JobClient:  map 33% reduce 0%
12/12/02 22:57:52 INFO mapred.JobClient:  map 52% reduce 0%
12/12/02 22:57:53 INFO mapred.JobClient:  map 69% reduce 0%
12/12/02 22:57:55 INFO mapred.JobClient:  map 83% reduce 0%
12/12/02 22:57:56 INFO mapred.JobClient:  map 97% reduce 0%
12/12/02 22:57:58 INFO mapred.JobClient:  map 100% reduce 0%
12/12/02 22:58:02 INFO mapred.JobClient:  map 100% reduce 66%
12/12/02 22:58:05 INFO mapred.JobClient:  map 100% reduce 100%
12/12/02 22:58:07 INFO mapred.JobClient: Job complete: job_201212022233_0002
12/12/02 22:58:07 INFO mapred.JobClient: Counters: 26
12/12/02 22:58:07 INFO mapred.JobClient:   Job Counters 
12/12/02 22:58:07 INFO mapred.JobClient:     Launched reduce tasks=1
12/12/02 22:58:07 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=42208
12/12/02 22:58:07 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
12/12/02 22:58:07 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
12/12/02 22:58:07 INFO mapred.JobClient:     Launched map tasks=3
12/12/02 22:58:07 INFO mapred.JobClient:     Data-local map tasks=3
12/12/02 22:58:07 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=15075
12/12/02 22:58:07 INFO mapred.JobClient:   FileSystemCounters
12/12/02 22:58:07 INFO mapred.JobClient:     FILE_BYTES_READ=79188878
12/12/02 22:58:07 INFO mapred.JobClient:     HDFS_BYTES_READ=155346075
12/12/02 22:58:07 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=118981181
12/12/02 22:58:07 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=3732775
12/12/02 22:58:07 INFO mapred.JobClient:   Map-Reduce Framework
12/12/02 22:58:07 INFO mapred.JobClient:     Map input records=2150524
12/12/02 22:58:07 INFO mapred.JobClient:     Reduce shuffle bytes=39594412
12/12/02 22:58:07 INFO mapred.JobClient:     Spilled Records=9591906
12/12/02 22:58:07 INFO mapred.JobClient:     Map output bytes=33199788
12/12/02 22:58:07 INFO mapred.JobClient:     CPU time spent (ms)=29030
12/12/02 22:58:07 INFO mapred.JobClient:     Total committed heap usage (bytes)=520908800
12/12/02 22:58:07 INFO mapred.JobClient:     Combine input records=0
12/12/02 22:58:07 INFO mapred.JobClient:     SPLIT_RAW_BYTES=336
12/12/02 22:58:07 INFO mapred.JobClient:     Reduce input records=3197302
12/12/02 22:58:07 INFO mapred.JobClient:     Reduce input groups=314914
12/12/02 22:58:07 INFO mapred.JobClient:     Combine output records=0
12/12/02 22:58:07 INFO mapred.JobClient:     Physical memory (bytes) snapshot=735645696
12/12/02 22:58:07 INFO mapred.JobClient:     Reduce output records=314914
12/12/02 22:58:07 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=4608991232
12/12/02 22:58:07 INFO mapred.JobClient:     Map output records=3197302
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -lsr /wordcount02
-rw-r--r--   3 root supergroup          0 2012-12-02 22:58 /wordcount02/_SUCCESS
drwxr-xr-x   - root supergroup          0 2012-12-02 22:57 /wordcount02/_logs
drwxr-xr-x   - root supergroup          0 2012-12-02 22:57 /wordcount02/_logs/history
-rw-r--r--   3 root supergroup      42983 2012-12-02 22:57 /wordcount02/_logs/history/hdmaster01_1354455187902_job_201212022233_0002_conf.xml
-rw-r--r--   3 root supergroup      17547 2012-12-02 22:57 /wordcount02/_logs/history/hdmaster01_1354455187902_job_201212022233_0002_root_wordcount
-rw-r--r--   3 root supergroup    3732775 2012-12-02 22:58 /wordcount02/part-r-00000
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -cat /wordcount02/part-r-00000 | sort -k2 -n -r | less
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop jar wordcount.jar WordCount /enwiki /wordcount03
12/12/02 23:01:25 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
12/12/02 23:01:25 INFO input.FileInputFormat: Total input paths to process : 2
12/12/02 23:01:25 WARN snappy.LoadSnappy: Snappy native library is available
12/12/02 23:01:25 INFO util.NativeCodeLoader: Loaded the native-hadoop library
12/12/02 23:01:25 INFO snappy.LoadSnappy: Snappy native library loaded
12/12/02 23:01:25 INFO mapred.JobClient: Running job: job_201212022233_0003
12/12/02 23:01:26 INFO mapred.JobClient:  map 0% reduce 0%
12/12/02 23:01:39 INFO mapred.JobClient:  map 25% reduce 0%
12/12/02 23:01:44 INFO mapred.JobClient:  map 60% reduce 0%
12/12/02 23:01:47 INFO mapred.JobClient:  map 64% reduce 0%
12/12/02 23:01:48 INFO mapred.JobClient:  map 71% reduce 0%
12/12/02 23:01:50 INFO mapred.JobClient:  map 80% reduce 0%
12/12/02 23:01:53 INFO mapred.JobClient:  map 92% reduce 0%
12/12/02 23:01:56 INFO mapred.JobClient:  map 100% reduce 0%
12/12/02 23:01:59 INFO mapred.JobClient:  map 100% reduce 33%
12/12/02 23:02:04 INFO mapred.JobClient:  map 100% reduce 100%
12/12/02 23:02:06 INFO mapred.JobClient: Job complete: job_201212022233_0003
12/12/02 23:02:06 INFO mapred.JobClient: Counters: 26
12/12/02 23:02:06 INFO mapred.JobClient:   Job Counters 
12/12/02 23:02:06 INFO mapred.JobClient:     Launched reduce tasks=1
12/12/02 23:02:06 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=70180
12/12/02 23:02:06 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
12/12/02 23:02:06 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
12/12/02 23:02:06 INFO mapred.JobClient:     Launched map tasks=4
12/12/02 23:02:06 INFO mapred.JobClient:     Data-local map tasks=4
12/12/02 23:02:06 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=23274
12/12/02 23:02:06 INFO mapred.JobClient:   FileSystemCounters
12/12/02 23:02:06 INFO mapred.JobClient:     FILE_BYTES_READ=79537299
12/12/02 23:02:06 INFO mapred.JobClient:     HDFS_BYTES_READ=156487287
12/12/02 23:02:06 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=119727444
12/12/02 23:02:06 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=3734805
12/12/02 23:02:06 INFO mapred.JobClient:   Map-Reduce Framework
12/12/02 23:02:06 INFO mapred.JobClient:     Map input records=2165951
12/12/02 23:02:06 INFO mapred.JobClient:     Reduce shuffle bytes=39942839
12/12/02 23:02:06 INFO mapred.JobClient:     Spilled Records=9648742
12/12/02 23:02:06 INFO mapred.JobClient:     Map output bytes=33491373
12/12/02 23:02:06 INFO mapred.JobClient:     CPU time spent (ms)=28300
12/12/02 23:02:06 INFO mapred.JobClient:     Total committed heap usage (bytes)=668807168
12/12/02 23:02:06 INFO mapred.JobClient:     Combine input records=0
12/12/02 23:02:06 INFO mapred.JobClient:     SPLIT_RAW_BYTES=449
12/12/02 23:02:06 INFO mapred.JobClient:     Reduce input records=3225720
12/12/02 23:02:06 INFO mapred.JobClient:     Reduce input groups=315026
12/12/02 23:02:06 INFO mapred.JobClient:     Combine output records=0
12/12/02 23:02:06 INFO mapred.JobClient:     Physical memory (bytes) snapshot=908521472
12/12/02 23:02:06 INFO mapred.JobClient:     Reduce output records=315026
12/12/02 23:02:06 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=5657382912
12/12/02 23:02:06 INFO mapred.JobClient:     Map output records=3225720
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -lsr /wordcount03
-rw-r--r--   3 root supergroup          0 2012-12-02 23:02 /wordcount03/_SUCCESS
drwxr-xr-x   - root supergroup          0 2012-12-02 23:01 /wordcount03/_logs
drwxr-xr-x   - root supergroup          0 2012-12-02 23:01 /wordcount03/_logs/history
-rw-r--r--   3 root supergroup      42966 2012-12-02 23:01 /wordcount03/_logs/history/hdmaster01_1354455187902_job_201212022233_0003_conf.xml
-rw-r--r--   3 root supergroup      19942 2012-12-02 23:01 /wordcount03/_logs/history/hdmaster01_1354455187902_job_201212022233_0003_root_wordcount
-rw-r--r--   3 root supergroup    3734805 2012-12-02 23:01 /wordcount03/part-r-00000
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# 
[root@hdclient01 wordcount]# hadoop fs -cat /wordcount03/part-r-00000 | sort -k2 -n -r | less
[root@hdclient01 wordcount]# 